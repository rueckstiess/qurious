model:
  base_model: 
    - gpt2
    - meta-llama/Llama-3.2-1B-Instruct
    # - meta-llama/Llama-3.2-3B-Instruct
    # - meta-llama/Llama-3.1-8B-Instruct
  device: auto
  lora_enabled: true
  lora_config:
    bias: none
    lora_alpha: 16
    lora_dropout: 0.05
    r: 8
    target_modules: all-linear
    task_type: CAUSAL_LM
paths:
  checkpoint_dir: ./checkpoints
  data_path: ../../data/grid_world_1k.jsonl
training:
  batch_size: 4
  epochs: 3
  eval_interval: 500
  learning_rate: 1e-4
  log_interval: 10
  max_eval_samples: 50
  max_grad_norm: 1.0
  save_interval: 1000
  scheduler_step_per_batch: true
