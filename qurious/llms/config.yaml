model:
  base_model: meta-llama/Llama-3.1-8B-Instruct
  device: auto
  lora_config:
    bias: none
    lora_alpha: [8, 16]
    lora_dropout: 0.05
    r: 8
    target_modules: all-linear
    task_type: CAUSAL_LM
  lora_enabled: true
paths:
  checkpoint_dir: ./checkpoints
  data_dir: ../../data
  log_dir: ./logs
  output_dir: ./outputs
training:
  batch_size: 4
  epochs: 10
  eval_interval: 500
  learning_rate: 1e-4
  log_interval: 10
  max_eval_samples: 50
  max_grad_norm: 1.0
  save_interval: 1000
  scheduler_step_per_batch: true
